{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import math \n",
    "import itertools\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# check folder exist if not create it\n",
    "def check_exist_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(\"Folder created: \", folder_path)\n",
    "    else:\n",
    "        print(\"Folder already exist: \", folder_path)\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "class Grapher:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "            This class is used to generate:\n",
    "                    1) the graph (in dictionary form) { source_node: [destination_node1, destination_node2]}\n",
    "                    2) the dataframe with relative_distances \n",
    "\n",
    "            Inputs: The class consists of a pandas dataframe consisting of cordinates for \n",
    "                    bounding boxe and the image of the invoice/receipt. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, data_fd):\n",
    "        self.filename = filename\n",
    "        self.data_fd = data_fd\n",
    "\n",
    "        file_path = os.path.join(self.data_fd, \"./csv_folder\", filename + '.csv')\n",
    "        image_path = os.path.join(self.data_fd, \"./image\", filename + '.png')\n",
    "        # interim_path = os.path.join(self.data_fd, \"./\", filename + '.csv')\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.image = cv2.imread(image_path)\n",
    "\n",
    "    def graph_formation(self, export_graph = False):\n",
    "\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ===========\n",
    "        Line formation:\n",
    "        1) Sort words based on Top coordinate:\n",
    "        2) Form lines as group of words which obeys the following:\n",
    "            Two words (W_a and W_b) are in same line if:\n",
    "                Top(W_a) <= Bottom(W_b) and Bottom(W_a) >= Top(W_b)\n",
    "        3) Sort words in each line based on Left coordinate\n",
    "\n",
    "        This ensures that words are read from top left corner of the image first, \n",
    "        going line by line from left to right and at last the final bottom right word of the page is read.\n",
    "    \n",
    "        Args: \n",
    "            df with words and cordinates (xmin,xmax,ymin,ymax)\n",
    "            image read into cv2\n",
    "        returns: \n",
    "            df with words arranged in orientation top to bottom and left to right, the line number for each word, index of the node connected to\n",
    "            on all directions top, bottom, right and left (if they exist and satisfy the parameters provided)\n",
    "\n",
    "        _____________________y axis______________________\n",
    "        |\n",
    "        |                       top    \n",
    "        x axis               ___________________\n",
    "        |              left | bounding box      |  right\n",
    "        |                   |___________________|           \n",
    "        |                       bottom \n",
    "        |\n",
    "        |\n",
    "\n",
    "\n",
    "        iterate through the rows twice to compare them.\n",
    "        remember that the axes are inverted.\n",
    "      \n",
    "        \"\"\"\n",
    "        df, image = self.df, self.image\n",
    "        \"\"\"\n",
    "        preprocessing the raw csv files to favorable df \n",
    "        \"\"\"\n",
    "\n",
    "        assert type(df) == pd.DataFrame,f'object_map should be of type \\\n",
    "            {pd.DataFrame}. Received {type(df)}'\n",
    "        assert type(image) == np.ndarray,f'image should be of type {np.ndarray} \\\n",
    "            . Received {type(image)}'\n",
    "        \n",
    "        assert 'xmin' in df.columns, '\"xmin\" not in object map'\n",
    "        assert 'xmax' in df.columns, '\"xmax\" not in object map'\n",
    "        assert 'ymin' in df.columns, '\"ymin\" not in object map'\n",
    "        assert 'ymax' in df.columns, '\"ymax\" not in object map'\n",
    "        assert 'Object' in df.columns, '\"Object\" column not in object map'\n",
    "\n",
    "        #remove empty spaces both in front and behind\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].str.strip()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        #further cleaning\n",
    "        df.dropna(inplace=True)\n",
    "        #sort from top to bottom\n",
    "        df.sort_values(by=['ymin'], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #subtracting ymax by 1 to eliminate ambiguity of boxes being in both left and right \n",
    "        df[\"ymax\"] = df[\"ymax\"].apply(lambda x: x - 1)\n",
    "        # df[\"ymax\"] = df[\"ymax\"].apply(lambda x: x)\n",
    "        \n",
    "        master = []\n",
    "        for idx, row in df.iterrows():\n",
    "            #flatten the nested list \n",
    "            flat_master = list(itertools.chain(*master))\n",
    "            #check to see if idx is in flat_master\n",
    "            if idx not in flat_master:\n",
    "                top_a = row['ymin']\n",
    "                bottom_a = row['ymax']         \n",
    "                #every line will atleast have the word in it\n",
    "                line = [idx]         \n",
    "                for idx_2, row_2 in df.iterrows():\n",
    "                    #check to see if idx_2 is in flat_master removes ambiguity\n",
    "                    #picks higher cordinate one. \n",
    "                    if idx_2 not in flat_master:\n",
    "                    #if not the same words\n",
    "                        if not idx == idx_2:\n",
    "                            top_b = row_2['ymin']\n",
    "                            bottom_b = row_2['ymax'] \n",
    "                            if (top_a <= bottom_b) and (bottom_a >= top_b): \n",
    "                                line.append(idx_2)\n",
    "                master.append(line)\n",
    "\n",
    "        df2 = pd.DataFrame({'words_indices': master, 'line_number':[x for x in range(1,len(master)+1)]})\n",
    "        #explode the list columns eg : [1,2,3]\n",
    "        df2 = df2.set_index('line_number').words_indices.apply(pd.Series).stack()\\\n",
    "                .reset_index(level=0).rename(columns={0:'words_indices'})\n",
    "        df2['words_indices'] = df2['words_indices'].astype('int')\n",
    "        #put the line numbers back to the list\n",
    "        final = df.merge(df2, left_on=df.index, right_on='words_indices')\n",
    "        final.drop('words_indices', axis=1, inplace=True)\n",
    "\n",
    "        \"\"\"\n",
    "        3) Sort words in each line based on Left coordinate\n",
    "        \"\"\"\n",
    "        final2 =final.sort_values(by=['line_number','xmin'],ascending=True)\\\n",
    "                .groupby('line_number')\\\n",
    "                .head(len(final))\\\n",
    "                .reset_index(drop=True)\n",
    "    \n",
    "        df = final2 \n",
    "\n",
    "        # print(\"FINAL 2 : \\n \", final2.head(3))\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Pseudocode:\n",
    "        1) Read words from each line starting from topmost line going towards bottommost line\n",
    "        2) For each word, perform the following:\n",
    "            - Check words which are in vertical projection with it.\n",
    "            - Calculate RD_l and RD_r for each of them \n",
    "            - Select nearest neighbour words in horizontal direction which have least magnitude of RD_l and RD_r, \n",
    "            provided that those words do not have an edge in that direciton.\n",
    "                    - In case, two words have same RD_l or RD_r, the word having higher top coordinate is chosen.\n",
    "            - Repeat steps from 2.1 to 2.3 similarly for retrieving nearest neighbour words in vertical direction by \n",
    "            taking horizontal projection, calculating RD_t and RD_b and choosing words having higher left co-ordinate\n",
    "            incase of ambiguity\n",
    "            - Draw edges between word and its 4 nearest neighbours if they are available.\n",
    "\n",
    "        Args: \n",
    "            df after lines properly aligned\n",
    "            \n",
    "        returns: \n",
    "            graph in the form of a dictionary, networkX graph, dataframe with \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        #horizontal edges formation\n",
    "        #print(df)\n",
    "        df.reset_index(inplace=True)\n",
    "        grouped = df.groupby('line_number')\n",
    "        #for undirected graph construction\n",
    "        horizontal_connections = {}\n",
    "        #left\n",
    "        left_connections = {}    \n",
    "        #right\n",
    "        right_connections = {}\n",
    "\n",
    "        for _,group in grouped:\n",
    "            a = group['index'].tolist()\n",
    "            b = group['index'].tolist()\n",
    "            horizontal_connection = {a[i]:a[i+1] for i in range(len(a)-1) }\n",
    "            #storing directional connections\n",
    "            right_dict_temp = {a[i]:{'right':a[i+1]} for i in range(len(a)-1) }\n",
    "            left_dict_temp = {b[i+1]:{'left':b[i]} for i in range(len(b)-1) }\n",
    "\n",
    "            #add the indices in the dataframes\n",
    "            for i in range(len(a)-1):\n",
    "                df.loc[df['index'] == a[i], 'right'] = int(a[i+1])\n",
    "                df.loc[df['index'] == a[i+1], 'left'] = int(a[i])\n",
    "        \n",
    "            left_connections.update(right_dict_temp)\n",
    "            right_connections.update(left_dict_temp)\n",
    "            horizontal_connections.update(horizontal_connection)\n",
    "\n",
    "        dic1,dic2 = left_connections, right_connections\n",
    "                \n",
    "        #verticle connections formation\n",
    "        bottom_connections = {}\n",
    "        top_connections = {}\n",
    "\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            if idx not in bottom_connections.keys():\n",
    "                right_a = row['xmax']\n",
    "                left_a = row['xmin']\n",
    "\n",
    "                for idx_2, row_2 in df.iterrows():\n",
    "                    #check for higher idx values \n",
    "\n",
    "                    if idx_2 not in bottom_connections.values() and idx < idx_2:\n",
    "                            right_b = row_2['xmax']\n",
    "                            left_b = row_2['xmin'] \n",
    "                            if (left_b <= right_a) and (right_b >= left_a): \n",
    "                                bottom_connections[idx] = idx_2                \n",
    "                                top_connections[idx_2] = idx\n",
    "\n",
    "                                #add it to the dataframe\n",
    "                                df.loc[df['index'] == idx , 'bottom'] = idx_2\n",
    "                                df.loc[df['index'] == idx_2, 'top'] = idx \n",
    "                                #print(bottom_connections)\n",
    "                                #once the condition is met, break the loop to reduce redundant time complexity\n",
    "                                break \n",
    "                        \n",
    "        #combining both \n",
    "        result = {}\n",
    "        dic1 = horizontal_connections\n",
    "        dic2 = bottom_connections\n",
    "\n",
    "        for key in (dic1.keys() | dic2.keys()):\n",
    "            if key in dic1: result.setdefault(key, []).append(dic1[key])\n",
    "            if key in dic2: result.setdefault(key, []).append(dic2[key])\n",
    "            \n",
    "        # print(result)\n",
    "        # print(dic1)\n",
    "        # print(dic2)\n",
    "\n",
    "        G = nx.from_dict_of_lists(result)\n",
    "        \n",
    "        if export_graph:\n",
    "\n",
    "            if not os.path.exists('./figures/graphs'):\n",
    "                os.makedirs('./figures/graphs')\t\t\t\n",
    "           \n",
    "            plot_path ='./figures/graphs/' + self.filename + 'plain_graph' '.jpg'\n",
    "            # print(plot_path)\n",
    "            layout = nx.kamada_kawai_layout(G)   \n",
    "            layout = nx.spring_layout(G)     \n",
    "            nx.draw(G, layout, with_labels=True)\n",
    "            plt.savefig(plot_path, format=\"PNG\", dpi=600)\n",
    "            #plt.show()\n",
    "\n",
    "        # connect with the interim file that has labels in it\n",
    "        df['labels'] = df[\"label\"].copy()\n",
    "        del df[\"label\"]\n",
    "\n",
    "        self.df = df \n",
    "        return G,result, df \n",
    "\n",
    "    #features calculation    \n",
    "    def get_text_features(self, df): \n",
    "        \"\"\"\n",
    "        gets text features \n",
    "\n",
    "        Args: df\n",
    "        Returns: n_lower, n_upper, n_spaces, n_alpha, n_numeric,n_special\n",
    "        \"\"\"\n",
    "        data = df['Object'].tolist()\n",
    "        \n",
    "        '''\n",
    "            Args:\n",
    "                df\n",
    "                \n",
    "            Returns: \n",
    "                character and word features\n",
    "                \n",
    "        '''\n",
    "        special_chars = ['&', '@', '#', '(',')','-','+', \n",
    "                    '=', '*', '%', '.', ',', '\\\\','/', \n",
    "                    '|', ':']\n",
    "\n",
    "        # character wise\n",
    "        n_lower, n_upper, n_spaces, n_alpha, n_numeric,n_special = [],[],[],[],[],[]\n",
    "\n",
    "        for words in data:\n",
    "            lower, upper,alpha,spaces,numeric,special = 0,0,0,0,0,0\n",
    "            for char in words: \n",
    "                if char.islower():\n",
    "                    lower += 1\n",
    "                # for upper letters \n",
    "                if char.isupper(): \n",
    "                    upper += 1\n",
    "                # for white spaces\n",
    "                if char.isspace():\n",
    "                    spaces += 1               \n",
    "                # for alphabetic chars\n",
    "                if char.isalpha():\n",
    "                    alpha += 1  \n",
    "                # for numeric chars\n",
    "                if char.isnumeric():\n",
    "                    numeric += 1                            \n",
    "                if char in special_chars:\n",
    "                    special += 1 \n",
    "\n",
    "            n_lower.append(lower)\n",
    "            n_upper.append(upper)\n",
    "            n_spaces.append(spaces)\n",
    "            n_alpha.append(alpha)\n",
    "            n_numeric.append(numeric)\n",
    "            n_special.append(special)\n",
    "            #features.append([n_lower, n_upper, n_spaces, n_alpha, n_numeric, n_digits])\n",
    "\n",
    "        df['n_upper'],df['n_alpha'],df['n_spaces'],\\\n",
    "        df['n_numeric'],df['n_special'] = n_upper, n_alpha, n_spaces, n_numeric,n_special\n",
    "        # self.df = df\n",
    "        return df\n",
    "\n",
    "    def relative_distance(self, export_document_graph = False):\n",
    "        \"\"\" \n",
    "        1) Calculates relative distances for each node in left, right, top  and bottom directions if they exist.\n",
    "        rd_l, rd_r = relative distances left , relative distances right. The distances are divided by image width\n",
    "        rd_t, rd_b = relative distances top , relative distances bottom. The distances are divided by image length\n",
    "\n",
    "        2) Exports the complete document graph for visualization\n",
    "\n",
    "        Args: \n",
    "            result dataframe from graph_formation()\n",
    "             \n",
    "        returns: \n",
    "            dataframe with features and exports document graph if prompted\n",
    "        \"\"\"\n",
    "\n",
    "        df, img = self.df, self.image\n",
    "        image_height, image_width = self.image.shape[0], self.image.shape[1]\n",
    "        plot_df = df.copy() \n",
    "\n",
    "        for index in df['index'].to_list():\n",
    "            right_index = df.loc[df['index'] == index, 'right'].values[0]\n",
    "            left_index = df.loc[df['index'] == index, 'left'].values[0]\n",
    "            bottom_index = df.loc[df['index'] == index, 'bottom'].values[0]\n",
    "            top_index = df.loc[df['index'] == index, 'top'].values[0]\n",
    "\n",
    "            #check if it is nan value \n",
    "            if np.isnan(right_index) == False: \n",
    "                right_word_left = df.loc[df['index'] == right_index, 'xmin'].values[0]\n",
    "                source_word_right = df.loc[df['index'] == index, 'xmax'].values[0]\n",
    "                df.loc[df['index'] == index, 'rd_r'] = (right_word_left - source_word_right)/image_width\n",
    "\n",
    "                \"\"\"\n",
    "                for plotting purposes\n",
    "                getting the mid point of the values to draw the lines for the graph\n",
    "                mid points of source and destination for the bounding boxes\n",
    "                \"\"\"\n",
    "                right_word_x_max = df.loc[df['index'] == right_index, 'xmax'].values[0]\n",
    "                right_word_y_max = df.loc[df['index'] == right_index, 'ymax'].values[0]\n",
    "                right_word_y_min = df.loc[df['index'] == right_index, 'ymin'].values[0]\n",
    "\n",
    "                df.loc[df['index'] == index, 'destination_x_hori'] = (right_word_x_max + right_word_left)/2\n",
    "                df.loc[df['index'] == index, 'destination_y_hori'] = (right_word_y_max + right_word_y_min)/2\n",
    "\n",
    "            if np.isnan(left_index) == False:\n",
    "                left_word_right = df.loc[df['index'] == left_index, 'xmax'].values[0]\n",
    "                source_word_left = df.loc[df['index'] == index, 'xmin'].values[0]\n",
    "                df.loc[df['index'] == index, 'rd_l'] = (left_word_right - source_word_left)/image_width\n",
    "            \n",
    "            if np.isnan(bottom_index) == False:\n",
    "                bottom_word_top = df.loc[df['index'] == bottom_index, 'ymin'].values[0]\n",
    "                source_word_bottom = df.loc[df['index'] == index, 'ymax'].values[0]\n",
    "                df.loc[df['index'] == index, 'rd_b'] = (bottom_word_top - source_word_bottom)/image_height\n",
    "\n",
    "                \"\"\"for plotting purposes\"\"\"\n",
    "                bottom_word_top_max = df.loc[df['index'] == bottom_index, 'ymax'].values[0]\n",
    "                bottom_word_x_max = df.loc[df['index'] == bottom_index, 'xmax'].values[0]\n",
    "                bottom_word_x_min = df.loc[df['index'] == bottom_index, 'xmin'].values[0]\n",
    "                df.loc[df['index'] == index, 'destination_y_vert'] = (bottom_word_top_max + bottom_word_top)/2\n",
    "                df.loc[df['index'] == index, 'destination_x_vert'] = (bottom_word_x_max + bottom_word_x_min)/2\n",
    "                \n",
    "            if np.isnan(top_index) == False:\n",
    "                top_word_bottom = df.loc[df['index'] == top_index, 'ymax'].values[0]\n",
    "                source_word_top = df.loc[df['index'] == index, 'ymin'].values[0]\n",
    "                df.loc[df['index'] == index, 'rd_t'] = (top_word_bottom - source_word_top)/image_height\n",
    "\n",
    "        #replace all tne NaN values with '0' meaning there is nothing in that direction\n",
    "        df[['rd_r','rd_b','rd_l','rd_t']] = df[['rd_r','rd_b','rd_l','rd_t']].fillna(0)\n",
    "\n",
    "        if export_document_graph:\n",
    "            for idx, row in df.iterrows():\n",
    "        #bounding box\n",
    "                cv2.rectangle(img, (row['xmin'], row['ymin']), (row['xmax'], row['ymax']), (0, 0, 255), 2)\n",
    "\n",
    "                if np.isnan(row['destination_x_vert']) == False:\n",
    "                    source_x = (row['xmax'] + row['xmin'])/2\n",
    "                    source_y = (row['ymax'] + row['ymin'])/2\n",
    "                    \n",
    "                    cv2.line(img, \n",
    "                            (int(source_x), int(source_y)),\n",
    "                            (int(row['destination_x_vert']), int(row['destination_y_vert'])), \n",
    "                            (0,255,0), 2)\n",
    "\n",
    "\n",
    "                    text = \"{:.3f}\".format(row['rd_b'])\n",
    "                    text_coordinates = ( int((row['destination_x_vert'] + source_x)/2) , int((row['destination_y_vert'] +source_y)/2))     \n",
    "                    cv2.putText(img, text, text_coordinates, cv2.FONT_HERSHEY_DUPLEX, 0.4, (255,0,0), 1)\n",
    "\n",
    "                    #text_coordinates = ((row['destination_x_vert'] + source_x)/2 , (row['destination_y_vert'] +source_y)/2)\n",
    "                \n",
    "                if np.isnan(row['destination_x_hori']) == False:\n",
    "                    source_x = (row['xmax'] + row['xmin'])/2\n",
    "                    source_y = (row['ymax'] + row['ymin'])/2\n",
    "\n",
    "                    cv2.line(img, \n",
    "                        (int(source_x), int(source_y)),\n",
    "                        (int(row['destination_x_hori']), int(row['destination_y_hori'])), \\\n",
    "                        (0,255,0), 2)\n",
    "\n",
    "                    text = \"{:.3f}\".format(row['rd_r'])\n",
    "                    text_coordinates = (int((row['destination_x_hori'] + source_x)/2) , int((row['destination_y_hori'] +source_y)/2))     \n",
    "                    cv2.putText(img, text, text_coordinates, cv2.FONT_HERSHEY_DUPLEX, 0.4, (255,0,0), 1)\n",
    "\n",
    "            # cv2.imshow(\"image\", img)\n",
    "            # cv2.waitKey(0)\n",
    "            # cv2.destroyAllWindows()\n",
    "                if not os.path.exists('../../figures/graphs'):\n",
    "                    os.makedirs('../../figures/graphs')\t\t\t\n",
    "                    \n",
    "                plot_path ='../../figures/graphs/' + self.filename + 'docu_graph' '.png'\n",
    "                cv2.imwrite(plot_path, img)\n",
    "   \n",
    "        #drop the unnecessary columns\n",
    "        df.drop(['destination_x_hori', 'destination_y_hori','destination_y_vert','destination_x_vert'], axis=1, inplace=True)\n",
    "        self.get_text_features(df)\n",
    "        return df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     file = '339'\n",
    "#     connect = Grapher(file)\n",
    "#     G,result, df = connect.graph_formation(export_graph=True)\n",
    "#     df = connect.relative_distance(export_document_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>Object</th>\n",
       "      <th>line_number</th>\n",
       "      <th>right</th>\n",
       "      <th>left</th>\n",
       "      <th>bottom</th>\n",
       "      <th>top</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>292</td>\n",
       "      <td>307</td>\n",
       "      <td>314</td>\n",
       "      <td>máu não trong bệnh mạch não (160-167f)</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diagnose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>272</td>\n",
       "      <td>732</td>\n",
       "      <td>293</td>\n",
       "      <td>Chấn đoán: T10 - Tăng huyết áp vô căn (nguyên ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>diagnose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>386</td>\n",
       "      <td>294</td>\n",
       "      <td>412</td>\n",
       "      <td>1) RENAPRIL 5MG 5mg</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>drugname</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>595</td>\n",
       "      <td>387</td>\n",
       "      <td>708</td>\n",
       "      <td>408</td>\n",
       "      <td>SL: 28 Viên</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>289</td>\n",
       "      <td>416</td>\n",
       "      <td>376</td>\n",
       "      <td>435</td>\n",
       "      <td>Sáng 1 Viên</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>438</td>\n",
       "      <td>281</td>\n",
       "      <td>465</td>\n",
       "      <td>2) NOVOXIM-500 0,5g</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>drugname</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>595</td>\n",
       "      <td>439</td>\n",
       "      <td>708</td>\n",
       "      <td>460</td>\n",
       "      <td>SL: 20 Viên</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>488</td>\n",
       "      <td>501</td>\n",
       "      <td>517</td>\n",
       "      <td>3) HOẠT HUYẾT DƯỠNG NÃO 150mg+20mg</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>drugname</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>595</td>\n",
       "      <td>491</td>\n",
       "      <td>708</td>\n",
       "      <td>512</td>\n",
       "      <td>SL: 20 Viên</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>289</td>\n",
       "      <td>519</td>\n",
       "      <td>376</td>\n",
       "      <td>539</td>\n",
       "      <td>Sáng 2 Viên</td>\n",
       "      <td>6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>625</td>\n",
       "      <td>518</td>\n",
       "      <td>700</td>\n",
       "      <td>535</td>\n",
       "      <td>Tối 2 Viên</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>484</td>\n",
       "      <td>633</td>\n",
       "      <td>621</td>\n",
       "      <td>651</td>\n",
       "      <td>Ngày 03/07/2019</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  xmin  ymin  xmax  ymax  \\\n",
       "0       0    26   292   307   314   \n",
       "1       1    28   272   732   293   \n",
       "2       2    46   386   294   412   \n",
       "3       3   595   387   708   408   \n",
       "4       4   289   416   376   435   \n",
       "5       5    44   438   281   465   \n",
       "6       6   595   439   708   460   \n",
       "7       7    45   488   501   517   \n",
       "8       8   595   491   708   512   \n",
       "9       9   289   519   376   539   \n",
       "10     10   625   518   700   535   \n",
       "11     11   484   633   621   651   \n",
       "\n",
       "                                               Object  line_number  right  \\\n",
       "0              máu não trong bệnh mạch não (160-167f)            1    1.0   \n",
       "1   Chấn đoán: T10 - Tăng huyết áp vô căn (nguyên ...            1    NaN   \n",
       "2                                 1) RENAPRIL 5MG 5mg            2    3.0   \n",
       "3                                         SL: 28 Viên            2    NaN   \n",
       "4                                         Sáng 1 Viên            3    NaN   \n",
       "5                                 2) NOVOXIM-500 0,5g            4    6.0   \n",
       "6                                         SL: 20 Viên            4    NaN   \n",
       "7                  3) HOẠT HUYẾT DƯỠNG NÃO 150mg+20mg            5    8.0   \n",
       "8                                         SL: 20 Viên            5    NaN   \n",
       "9                                         Sáng 2 Viên            6   10.0   \n",
       "10                                         Tối 2 Viên            6    NaN   \n",
       "11                                    Ngày 03/07/2019            7    NaN   \n",
       "\n",
       "    left  bottom  top    labels  \n",
       "0    NaN     1.0  NaN  diagnose  \n",
       "1    0.0     2.0  0.0  diagnose  \n",
       "2    NaN     4.0  1.0  drugname  \n",
       "3    2.0     6.0  NaN  quantity  \n",
       "4    NaN     7.0  2.0     usage  \n",
       "5    NaN     NaN  NaN  drugname  \n",
       "6    5.0     8.0  3.0  quantity  \n",
       "7    NaN     9.0  4.0  drugname  \n",
       "8    7.0    10.0  6.0  quantity  \n",
       "9    NaN     NaN  7.0     usage  \n",
       "10   9.0     NaN  8.0     usage  \n",
       "11   NaN     NaN  NaN      date  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = 'VAIPE_P_TRAIN_0'\n",
    "\n",
    "data_fd =  \"/home/long/Desktop/TestInProject/data_vaipe/public_train/prescription\"\n",
    "connect = Grapher(file1, data_fd)\n",
    "G,result, df1 = connect.graph_formation(export_graph=False)\n",
    "# print(len(df1))\n",
    "print(G.nodes)\n",
    "df1\n",
    "# df1.info\n",
    "# df = connect.relative_distance(export_document_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>Object</th>\n",
       "      <th>line_number</th>\n",
       "      <th>right</th>\n",
       "      <th>left</th>\n",
       "      <th>bottom</th>\n",
       "      <th>...</th>\n",
       "      <th>labels</th>\n",
       "      <th>rd_b</th>\n",
       "      <th>rd_t</th>\n",
       "      <th>rd_r</th>\n",
       "      <th>rd_l</th>\n",
       "      <th>n_upper</th>\n",
       "      <th>n_alpha</th>\n",
       "      <th>n_spaces</th>\n",
       "      <th>n_numeric</th>\n",
       "      <th>n_special</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>272</td>\n",
       "      <td>713</td>\n",
       "      <td>293</td>\n",
       "      <td>Chấn đoán: E11 - Bệnh đái tháo đường không phụ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>diagnose</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>294</td>\n",
       "      <td>65</td>\n",
       "      <td>311</td>\n",
       "      <td>phát)</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>diagnose</td>\n",
       "      <td>0.070093</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>386</td>\n",
       "      <td>326</td>\n",
       "      <td>412</td>\n",
       "      <td>1) GLUCOFAST 850 850mg</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>drugname</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>-0.070093</td>\n",
       "      <td>0.353947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>595</td>\n",
       "      <td>387</td>\n",
       "      <td>708</td>\n",
       "      <td>408</td>\n",
       "      <td>SL: 60 Viên</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>quantity</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.353947</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>289</td>\n",
       "      <td>416</td>\n",
       "      <td>376</td>\n",
       "      <td>435</td>\n",
       "      <td>Sáng 1 Viên</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>usage</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>-0.003738</td>\n",
       "      <td>0.181579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  xmin  ymin  xmax  ymax  \\\n",
       "0      0    28   272   713   293   \n",
       "1      1    26   294    65   311   \n",
       "2      2    46   386   326   412   \n",
       "3      3   595   387   708   408   \n",
       "4      4   289   416   376   435   \n",
       "\n",
       "                                              Object  line_number  right  \\\n",
       "0  Chấn đoán: E11 - Bệnh đái tháo đường không phụ...            1    NaN   \n",
       "1                                              phát)            2    NaN   \n",
       "2                             1) GLUCOFAST 850 850mg            3    3.0   \n",
       "3                                        SL: 60 Viên            3    NaN   \n",
       "4                                        Sáng 1 Viên            4    5.0   \n",
       "\n",
       "   left  bottom  ...    labels      rd_b      rd_t      rd_r      rd_l  \\\n",
       "0   NaN     1.0  ...  diagnose  0.000935  0.000000  0.000000  0.000000   \n",
       "1   NaN     2.0  ...  diagnose  0.070093 -0.000935  0.000000  0.000000   \n",
       "2   NaN     4.0  ...  drugname  0.003738 -0.070093  0.353947  0.000000   \n",
       "3   2.0     5.0  ...  quantity  0.005607  0.000000  0.000000 -0.353947   \n",
       "4   NaN     6.0  ...     usage  0.002804 -0.003738  0.181579  0.000000   \n",
       "\n",
       "   n_upper  n_alpha  n_spaces  n_numeric  n_special  \n",
       "0        4       67        18          5          5  \n",
       "1        0        4         0          0          1  \n",
       "2        9       11         3          7          1  \n",
       "3        3        6         2          2          1  \n",
       "4        2        8         2          1          0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connect.relative_distance().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1143fb826fcf47048768ee0ec3152e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1173.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-60cd7dc1ae85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files_raw_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mconnect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGrapher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_fd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_formation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# print(\"error\", count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-3e722557e849>\u001b[0m in \u001b[0;36mgraph_formation\u001b[0;34m(self, export_graph)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;31m#every line will atleast have the word in it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0midx_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                     \u001b[0;31m#check to see if idx_2 is in flat_master removes ambiguity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0;31m#picks higher cordinate one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   1511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mklass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3033\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3034\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_block_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_fd =  \"/home/long/Desktop/TestInProject/data_vaipe/public_train/prescription/\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get all file name in folder\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# get file name in folder and put them in a list\n",
    "path_csv_box = '/home/long/Desktop/TestInProject/data_vaipe/public_train/prescription/csv_folder/'\n",
    "all_files_raw_box = glob.glob(path_csv_box + \"*.csv\")\n",
    "all_files_raw_box.sort()\n",
    "# just get number in string\n",
    "all_files_raw_box = [i.split('/')[-1].split('.')[0] for i in all_files_raw_box]\n",
    "# all_files_raw_box = [i for i in all_files_raw_box]\n",
    "\n",
    "# print(all_files_raw_box)\n",
    "\n",
    "count = []\n",
    "\n",
    "for file in tqdm_notebook(all_files_raw_box):\n",
    "    connect = Grapher(file, data_fd)\n",
    "    G,result, df = connect.graph_formation(export_graph=False)\n",
    "    if len(df) != len(G):\n",
    "        # print(\"error\", count)\n",
    "        count.append(file)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VAIPE_P_TRAIN_0',\n",
       " 'VAIPE_P_TRAIN_1007',\n",
       " 'VAIPE_P_TRAIN_1008',\n",
       " 'VAIPE_P_TRAIN_1009',\n",
       " 'VAIPE_P_TRAIN_1010',\n",
       " 'VAIPE_P_TRAIN_1020',\n",
       " 'VAIPE_P_TRAIN_1023',\n",
       " 'VAIPE_P_TRAIN_1026',\n",
       " 'VAIPE_P_TRAIN_1028',\n",
       " 'VAIPE_P_TRAIN_103',\n",
       " 'VAIPE_P_TRAIN_1036',\n",
       " 'VAIPE_P_TRAIN_1063',\n",
       " 'VAIPE_P_TRAIN_1065',\n",
       " 'VAIPE_P_TRAIN_1069',\n",
       " 'VAIPE_P_TRAIN_1082',\n",
       " 'VAIPE_P_TRAIN_1083',\n",
       " 'VAIPE_P_TRAIN_1091',\n",
       " 'VAIPE_P_TRAIN_1096',\n",
       " 'VAIPE_P_TRAIN_1098',\n",
       " 'VAIPE_P_TRAIN_1101',\n",
       " 'VAIPE_P_TRAIN_1105',\n",
       " 'VAIPE_P_TRAIN_1113',\n",
       " 'VAIPE_P_TRAIN_1116',\n",
       " 'VAIPE_P_TRAIN_112',\n",
       " 'VAIPE_P_TRAIN_1125',\n",
       " 'VAIPE_P_TRAIN_1130',\n",
       " 'VAIPE_P_TRAIN_1131',\n",
       " 'VAIPE_P_TRAIN_1132',\n",
       " 'VAIPE_P_TRAIN_1133',\n",
       " 'VAIPE_P_TRAIN_1144',\n",
       " 'VAIPE_P_TRAIN_1146',\n",
       " 'VAIPE_P_TRAIN_1147',\n",
       " 'VAIPE_P_TRAIN_1167',\n",
       " 'VAIPE_P_TRAIN_1170',\n",
       " 'VAIPE_P_TRAIN_119',\n",
       " 'VAIPE_P_TRAIN_120',\n",
       " 'VAIPE_P_TRAIN_130',\n",
       " 'VAIPE_P_TRAIN_137',\n",
       " 'VAIPE_P_TRAIN_139',\n",
       " 'VAIPE_P_TRAIN_141',\n",
       " 'VAIPE_P_TRAIN_145',\n",
       " 'VAIPE_P_TRAIN_146',\n",
       " 'VAIPE_P_TRAIN_153',\n",
       " 'VAIPE_P_TRAIN_156',\n",
       " 'VAIPE_P_TRAIN_157',\n",
       " 'VAIPE_P_TRAIN_158',\n",
       " 'VAIPE_P_TRAIN_159',\n",
       " 'VAIPE_P_TRAIN_16',\n",
       " 'VAIPE_P_TRAIN_160',\n",
       " 'VAIPE_P_TRAIN_161',\n",
       " 'VAIPE_P_TRAIN_162',\n",
       " 'VAIPE_P_TRAIN_163',\n",
       " 'VAIPE_P_TRAIN_164',\n",
       " 'VAIPE_P_TRAIN_165',\n",
       " 'VAIPE_P_TRAIN_168',\n",
       " 'VAIPE_P_TRAIN_17',\n",
       " 'VAIPE_P_TRAIN_18',\n",
       " 'VAIPE_P_TRAIN_181',\n",
       " 'VAIPE_P_TRAIN_19',\n",
       " 'VAIPE_P_TRAIN_192',\n",
       " 'VAIPE_P_TRAIN_201',\n",
       " 'VAIPE_P_TRAIN_218',\n",
       " 'VAIPE_P_TRAIN_228',\n",
       " 'VAIPE_P_TRAIN_229',\n",
       " 'VAIPE_P_TRAIN_230',\n",
       " 'VAIPE_P_TRAIN_231',\n",
       " 'VAIPE_P_TRAIN_232',\n",
       " 'VAIPE_P_TRAIN_233',\n",
       " 'VAIPE_P_TRAIN_235',\n",
       " 'VAIPE_P_TRAIN_238',\n",
       " 'VAIPE_P_TRAIN_242',\n",
       " 'VAIPE_P_TRAIN_243',\n",
       " 'VAIPE_P_TRAIN_244',\n",
       " 'VAIPE_P_TRAIN_249',\n",
       " 'VAIPE_P_TRAIN_250',\n",
       " 'VAIPE_P_TRAIN_252',\n",
       " 'VAIPE_P_TRAIN_260',\n",
       " 'VAIPE_P_TRAIN_276',\n",
       " 'VAIPE_P_TRAIN_295',\n",
       " 'VAIPE_P_TRAIN_296',\n",
       " 'VAIPE_P_TRAIN_297',\n",
       " 'VAIPE_P_TRAIN_313',\n",
       " 'VAIPE_P_TRAIN_320',\n",
       " 'VAIPE_P_TRAIN_321',\n",
       " 'VAIPE_P_TRAIN_322',\n",
       " 'VAIPE_P_TRAIN_323',\n",
       " 'VAIPE_P_TRAIN_324',\n",
       " 'VAIPE_P_TRAIN_329',\n",
       " 'VAIPE_P_TRAIN_351',\n",
       " 'VAIPE_P_TRAIN_354',\n",
       " 'VAIPE_P_TRAIN_375',\n",
       " 'VAIPE_P_TRAIN_396',\n",
       " 'VAIPE_P_TRAIN_397',\n",
       " 'VAIPE_P_TRAIN_40',\n",
       " 'VAIPE_P_TRAIN_404',\n",
       " 'VAIPE_P_TRAIN_406',\n",
       " 'VAIPE_P_TRAIN_407',\n",
       " 'VAIPE_P_TRAIN_408',\n",
       " 'VAIPE_P_TRAIN_409',\n",
       " 'VAIPE_P_TRAIN_41',\n",
       " 'VAIPE_P_TRAIN_410',\n",
       " 'VAIPE_P_TRAIN_42',\n",
       " 'VAIPE_P_TRAIN_429',\n",
       " 'VAIPE_P_TRAIN_43',\n",
       " 'VAIPE_P_TRAIN_449',\n",
       " 'VAIPE_P_TRAIN_455',\n",
       " 'VAIPE_P_TRAIN_456',\n",
       " 'VAIPE_P_TRAIN_457',\n",
       " 'VAIPE_P_TRAIN_46',\n",
       " 'VAIPE_P_TRAIN_470',\n",
       " 'VAIPE_P_TRAIN_475',\n",
       " 'VAIPE_P_TRAIN_477']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>Object</th>\n",
       "      <th>line_number</th>\n",
       "      <th>right</th>\n",
       "      <th>left</th>\n",
       "      <th>bottom</th>\n",
       "      <th>top</th>\n",
       "      <th>labels</th>\n",
       "      <th>n_upper</th>\n",
       "      <th>n_alpha</th>\n",
       "      <th>n_spaces</th>\n",
       "      <th>n_numeric</th>\n",
       "      <th>n_special</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>272</td>\n",
       "      <td>322</td>\n",
       "      <td>291</td>\n",
       "      <td>Chẩn đoán: L02 - Áp xe da, nhọt, nhọt cụm nhau</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diagnose</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>387</td>\n",
       "      <td>349</td>\n",
       "      <td>411</td>\n",
       "      <td>1) CEFADROXIL 500MG 0,5g</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>drugname</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>595</td>\n",
       "      <td>387</td>\n",
       "      <td>708</td>\n",
       "      <td>408</td>\n",
       "      <td>SL: 20 Viên</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>quantity</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>416</td>\n",
       "      <td>376</td>\n",
       "      <td>435</td>\n",
       "      <td>Sáng 2 Viên</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>usage</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>625</td>\n",
       "      <td>414</td>\n",
       "      <td>700</td>\n",
       "      <td>432</td>\n",
       "      <td>Tối 2 Viên</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>usage</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  xmin  ymin  xmax  ymax  \\\n",
       "0      0    28   272   322   291   \n",
       "1      1    46   387   349   411   \n",
       "2      2   595   387   708   408   \n",
       "3      3   289   416   376   435   \n",
       "4      4   625   414   700   432   \n",
       "\n",
       "                                           Object  line_number  right  left  \\\n",
       "0  Chẩn đoán: L02 - Áp xe da, nhọt, nhọt cụm nhau            1    NaN   NaN   \n",
       "1                        1) CEFADROXIL 500MG 0,5g            2    2.0   NaN   \n",
       "2                                     SL: 20 Viên            2    NaN   1.0   \n",
       "3                                     Sáng 2 Viên            3    4.0   NaN   \n",
       "4                                      Tối 2 Viên            3    NaN   3.0   \n",
       "\n",
       "   bottom  top    labels  n_upper  n_alpha  n_spaces  n_numeric  n_special  \n",
       "0     1.0  NaN  diagnose        3       30        10          2          4  \n",
       "1     3.0  0.0  drugname       12       13         3          6          2  \n",
       "2     4.0  NaN  quantity        3        6         2          2          1  \n",
       "3     5.0  1.0     usage        2        8         2          1          0  \n",
       "4     6.0  2.0     usage        2        7         2          1          0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connect.get_text_features(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>Object</th>\n",
       "      <th>line_number</th>\n",
       "      <th>right</th>\n",
       "      <th>left</th>\n",
       "      <th>bottom</th>\n",
       "      <th>...</th>\n",
       "      <th>labels</th>\n",
       "      <th>n_upper</th>\n",
       "      <th>n_alpha</th>\n",
       "      <th>n_spaces</th>\n",
       "      <th>n_numeric</th>\n",
       "      <th>n_special</th>\n",
       "      <th>rd_b</th>\n",
       "      <th>rd_r</th>\n",
       "      <th>rd_t</th>\n",
       "      <th>rd_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>272</td>\n",
       "      <td>322</td>\n",
       "      <td>291</td>\n",
       "      <td>Chẩn đoán: L02 - Áp xe da, nhọt, nhọt cụm nhau</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>diagnose</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.089720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>387</td>\n",
       "      <td>349</td>\n",
       "      <td>411</td>\n",
       "      <td>1) CEFADROXIL 500MG 0,5g</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>drugname</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.323684</td>\n",
       "      <td>-0.089720</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>595</td>\n",
       "      <td>387</td>\n",
       "      <td>708</td>\n",
       "      <td>408</td>\n",
       "      <td>SL: 20 Viên</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>quantity</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.323684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>289</td>\n",
       "      <td>416</td>\n",
       "      <td>376</td>\n",
       "      <td>435</td>\n",
       "      <td>Sáng 2 Viên</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>usage</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.327632</td>\n",
       "      <td>-0.004673</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>625</td>\n",
       "      <td>414</td>\n",
       "      <td>700</td>\n",
       "      <td>432</td>\n",
       "      <td>Tối 2 Viên</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>usage</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005607</td>\n",
       "      <td>-0.327632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  xmin  ymin  xmax  ymax  \\\n",
       "0      0    28   272   322   291   \n",
       "1      1    46   387   349   411   \n",
       "2      2   595   387   708   408   \n",
       "3      3   289   416   376   435   \n",
       "4      4   625   414   700   432   \n",
       "\n",
       "                                           Object  line_number  right  left  \\\n",
       "0  Chẩn đoán: L02 - Áp xe da, nhọt, nhọt cụm nhau            1    NaN   NaN   \n",
       "1                        1) CEFADROXIL 500MG 0,5g            2    2.0   NaN   \n",
       "2                                     SL: 20 Viên            2    NaN   1.0   \n",
       "3                                     Sáng 2 Viên            3    4.0   NaN   \n",
       "4                                      Tối 2 Viên            3    NaN   3.0   \n",
       "\n",
       "   bottom  ...    labels n_upper  n_alpha  n_spaces  n_numeric  n_special  \\\n",
       "0     1.0  ...  diagnose       3       30        10          2          4   \n",
       "1     3.0  ...  drugname      12       13         3          6          2   \n",
       "2     4.0  ...  quantity       3        6         2          2          1   \n",
       "3     5.0  ...     usage       2        8         2          1          0   \n",
       "4     6.0  ...     usage       2        7         2          1          0   \n",
       "\n",
       "       rd_b      rd_r      rd_t      rd_l  \n",
       "0  0.089720  0.000000  0.000000  0.000000  \n",
       "1  0.004673  0.323684 -0.089720  0.000000  \n",
       "2  0.005607  0.000000  0.000000 -0.323684  \n",
       "3  0.002804  0.327632 -0.004673  0.000000  \n",
       "4  0.006542  0.000000 -0.005607 -0.327632  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connect.relative_distance().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:61: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf4a82433fe44b4b8cec266d8c86e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1172.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Folder created:  /home/long/Desktop/TestInProject/data_vaipe/public_train/prescription/processed\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch_geometric\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from bpemb import BPEmb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "import os \n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bpemb_en = BPEmb(lang=\"en\", dim=100)\n",
    "sent_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens', device='cuda:0')\n",
    "\n",
    "\n",
    "data_fd =  \"/home/long/Desktop/TestInProject/data_vaipe/public_train/prescription/\"\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def make_sent_bert_features(text):\n",
    "    emb = sent_model.encode([text])[0]\n",
    "    return emb\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    file_path_list =  glob.glob(os.path.join(path, '*'))\n",
    "    # get file name by os.path.basename\n",
    "    file_name_list = [os.path.basename(file_path) for file_path in file_path_list]\n",
    "    # remove .csv from file name\n",
    "    file_name_list = [file_name.split('.')[0] for file_name in file_name_list]\n",
    "    return file_name_list\n",
    "\n",
    "def get_data(save_fd):\n",
    "    \"\"\"\n",
    "    returns one big graph with unconnected graphs with the following:\n",
    "    - x (Tensor, optional) – Node feature matrix with shape [num_nodes, num_node_features]. (default: None)\n",
    "    - edge_index (LongTensor, optional) – Graph connectivity in COO format with shape [2, num_edges]. (default: None)\n",
    "    - edge_attr (Tensor, optional) – Edge feature matrix with shape [num_edges, num_edge_features]. (default: None)\n",
    "    - y (Tensor, optional) – Graph or node targets with arbitrary shape. (default: None)\n",
    "    - validation mask, training mask and testing mask \n",
    "    \"\"\"\n",
    "    path = \"/home/long/Desktop/TestInProject/data_vaipe/public_train/prescription/csv_folder/\"\n",
    "    \n",
    "    \n",
    "    # files = [i.split('.')[0] for i in os.listdir(path)]\n",
    "    files = listdir_nohidden(path)\n",
    "\n",
    "    files.sort()\n",
    "    all_files = files[1:]\n",
    "\n",
    "    list_of_graphs = []\n",
    "    train_list_of_graphs, test_list_of_graphs = [], []\n",
    "\n",
    "    files = all_files.copy()\n",
    "    random.shuffle(files)\n",
    "\n",
    "    \"\"\"Resulting in 550 receipts for training\"\"\"\n",
    "    training, testing = files[:990], files[990:]\n",
    "    # training, testing = files, files\n",
    "\n",
    "    for file in tqdm_notebook(all_files):\n",
    "        # print(file)\n",
    "        # break\n",
    "\n",
    "        connect = Grapher(file, data_fd)\n",
    "        # try:\n",
    "        G,_,_ = connect.graph_formation()\n",
    "        # except:\n",
    "        #     print(\" lOOIXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
    "        #     print(\"file name  = \", file) \n",
    "        #     continue\n",
    "\n",
    "        df = connect.relative_distance() \n",
    "\n",
    "        individual_data = from_networkx(G)\n",
    "\n",
    "        feature_cols = ['rd_b', 'rd_r', 'rd_t', 'rd_l','line_number', \\\n",
    "                'n_upper', 'n_alpha', 'n_spaces', 'n_numeric','n_special']\n",
    "\n",
    "        text_features = np.array(df[\"Object\"].map(make_sent_bert_features).tolist()).astype(np.float32)\n",
    "        numeric_features = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "        features = np.concatenate((numeric_features, text_features), axis=1) \n",
    "        features = torch.tensor(features)\n",
    "\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].str.strip()\n",
    "            except AttributeError as e:\n",
    "                pass\n",
    "\n",
    "        # ****************************************************************************************\n",
    "        # print(df)\n",
    "        # ****************************************************************************************\n",
    "\n",
    "        df['labels'] = df['labels'].fillna('undefined')\n",
    "        df.loc[df['labels'] == 'drugname', 'num_labels'] = 1\n",
    "        df.loc[df['labels'] == 'quantity', 'num_labels'] = 2\n",
    "        df.loc[df['labels'] == 'date', 'num_labels'] = 3\n",
    "        df.loc[df['labels'] == 'usage', 'num_labels'] = 4\n",
    "        df.loc[df['labels'] == 'undefined', 'num_labels'] = 5\n",
    "        # fillna df.['num_labels'] by 5\n",
    "        df['num_labels'] = df['num_labels'].fillna(5)\n",
    " \n",
    "        # assert df['num_labels'].isnull().values.any() == False, f'labeling error! Invalid label(s) present in {file}.csv'\n",
    "        # print(\"####################################3\")\n",
    "        # print(df.columns)\n",
    "        # print(df.iloc[7])\n",
    "\n",
    "        \n",
    "        assert df['num_labels'].isnull().values.any() == False, f'labeling error! Invalid label(s) present in {file}.csv'\n",
    "        labels = torch.tensor(df['num_labels'].values.astype(np.int64))\n",
    "        text = df['Object'].values\n",
    "\n",
    "        # ****************************************************************************************\n",
    "        individual_data.x = features\n",
    "        individual_data.y = labels\n",
    "        individual_data.text = text\n",
    "        individual_data.img_id = file\n",
    "\n",
    "        if file in training:\n",
    "            train_list_of_graphs.append(individual_data)\n",
    "        elif file in testing:\n",
    "            test_list_of_graphs.append(individual_data)\n",
    "                \n",
    "    train_data = torch_geometric.data.Batch.from_data_list(train_list_of_graphs)\n",
    "    train_data.edge_attr = None\n",
    "    test_data = torch_geometric.data.Batch.from_data_list(test_list_of_graphs)\n",
    "    test_data.edge_attr = None\n",
    "\n",
    "    check_exist_folder(save_fd)\n",
    "    \n",
    "\n",
    "    torch.save(train_data, os.path.join(save_fd, 'train_data.dataset'))\n",
    "    torch.save(test_data, os.path.join(save_fd, 'test_data.dataset'))\n",
    "\n",
    "save_fd = \"/home/long/Desktop/TestInProject/data_vaipe/public_train/prescription/\"\n",
    "get_data(save_fd=save_fd + \"processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# import class ChebConv\n",
    "from torch_geometric.nn import ChebConv\n",
    "# import class GraphConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "# import class GCNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# import class torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class InvoiceGCN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, chebnet=False, n_classes=5, dropout_rate=0.2, K=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        if chebnet:\n",
    "            self.conv1 = ChebConv(self.input_dim, 64, K=K)\n",
    "            self.conv2 = ChebConv(64, 32, K=K)\n",
    "            self.conv3 = ChebConv(32, 16, K=K)\n",
    "            self.conv4 = ChebConv(16, self.n_classes, K=K)\n",
    "        else:\n",
    "            self.conv1 = GCNConv(self.first_dim, 64, improved=True, cached=True)\n",
    "            self.conv2 = GCNConv(64, 32, improved=True, cached=True)\n",
    "            self.conv3 = GCNConv(32, 16, improved=True, cached=True)\n",
    "            self.conv4 = GCNConv(16, self.n_classes, improved=True, cached=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # for transductive setting with full-batch update\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = F.dropout(F.relu(self.conv1(x, edge_index, edge_weight)), p=self.dropout_rate, training=self.training)\n",
    "        x = F.dropout(F.relu(self.conv2(x, edge_index, edge_weight)), p=self.dropout_rate, training=self.training)\n",
    "        x = F.dropout(F.relu(self.conv3(x, edge_index, edge_weight)), p=self.dropout_rate, training=self.training)\n",
    "        x = self.conv4(x, edge_index, edge_weight)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# import confution matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import clasiification report\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "\n",
    "save_fd = \"/home/long/Desktop/TestInProject/data_vaipe/public_train/prescription/processed\"\n",
    "train_data, test_data = torch.load(os.path.join(save_fd, 'train_data.dataset')), torch.load(os.path.join(save_fd, 'test_data.dataset'))\n",
    "\n",
    "model = InvoiceGCN(input_dim=train_data.x.shape[1], chebnet=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=0.001, weight_decay=0.9\n",
    ")\n",
    "train_data = train_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "# class weights for imbalanced data\n",
    "_class_weights = compute_class_weight(class_weight= \"balanced\", classes= train_data.y.unique().cpu().numpy(), y = train_data.y.cpu().numpy())\n",
    "# print(_class_weights)\n",
    "\n",
    "no_epochs = 2000\n",
    "for epoch in range(1, no_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # NOTE: just use boolean indexing to filter out test data, and backward after that!\n",
    "    # the same holds true with test data :D\n",
    "    # https://github.com/rusty1s/pytorch_geometric/issues/1928\n",
    "    loss = F.nll_loss(\n",
    "        model(train_data), train_data.y - 1, weight=torch.FloatTensor(_class_weights).to(device)\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # calculate acc on 5 classes\n",
    "    with torch.no_grad():\n",
    "        if epoch % 200 == 0:\n",
    "            model.eval()\n",
    "\n",
    "            # forward model\n",
    "            for index, name in enumerate(['train', 'test']):\n",
    "                _data = eval(\"{}_data\".format(name))\n",
    "                y_pred = model(_data).max(dim=1)[1]\n",
    "                y_true = (_data.y - 1)\n",
    "                acc = y_pred.eq(y_true).sum().item() / y_pred.shape[0]\n",
    "\n",
    "                y_pred = y_pred.cpu().numpy()\n",
    "                y_true = y_true.cpu().numpy()\n",
    "                print(\"\\t{} acc: {}\".format(name, acc))\n",
    "                # confusion matrix\n",
    "                if name == 'test':\n",
    "                    cm = confusion_matrix(y_true, y_pred)\n",
    "                    class_accs = cm.diagonal() / cm.sum(axis=1)\n",
    "                    print(classification_report(y_true, y_pred))\n",
    "\n",
    "            loss_val = F.nll_loss(model(test_data), test_data.y - 1\n",
    "            )\n",
    "            fmt_log = \"Epoch: {:03d}, train_loss:{:.4f}, val_loss:{:.4f}\"\n",
    "            print(fmt_log.format(epoch, loss, loss_val))\n",
    "            print(\">\" * 50)\n",
    "\n",
    "\n",
    "# ****************************************************************************************\n",
    "# save jtit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(edge_index=[2, 14620], num_nodes=6568, x=[6678, 778], y=[6678], text=[622], img_id=[622], batch=[6568], ptr=[623])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm in notebook\n",
    "from tqdm import tqdm\n",
    "import time \n",
    "# import Counter\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "\n",
    "\n",
    "img_fd = \"/home/long/Desktop/TestInProject/data_vaipe/public_train/prescription/image/\"\n",
    "test_output_fd = \"/home/long/Desktop/TestInProject/data_vaipe/output_folder/\"\n",
    "\n",
    "# shutil.rmtree(test_output_fd)\n",
    "if not os.path.exists(test_output_fd):\n",
    "    os.mkdir(test_output_fd)\n",
    "    print(\"Hello\")\n",
    "\n",
    "def make_info(img_id):\n",
    "    connect = Grapher(img_id, data_fd)\n",
    "    G, _, _ = connect.graph_formation()\n",
    "    df = connect.relative_distance()\n",
    "    individual_data = from_networkx(G)\n",
    "    print(\"################################################################\")\n",
    "    # print(img_fd + img_id + \".png\")\n",
    "    print(\"################################################################\")\n",
    "    # read image png to ipg\n",
    "\n",
    "    img = cv2.imread(os.path.join(img_fd, \"{}.png\".format(img_id)))\n",
    "    # print(img)\n",
    "    # img = cv2.imread(os.path.join(img_fd, \"{}.png\".format(img_id)))[:, :, ::-1]\n",
    "\n",
    "\n",
    "    return G, df, individual_data, img\n",
    "\n",
    "y_preds = model(test_data).max(dim=1)[1].cpu().numpy()\n",
    "LABELS = [\"drugname\", \"quantity\", \"date\", \"usage\", \"other\"]\n",
    "test_batch = test_data.batch.cpu().numpy()\n",
    "indexes = range(len(test_data.img_id))\n",
    "for index in tqdm(indexes):\n",
    "    start = time.time()\n",
    "    img_id = test_data.img_id[index]  # not ordering by img_id\n",
    "    sample_indexes = np.where(test_batch == index)[0]\n",
    "    y_pred = y_preds[sample_indexes]\n",
    "\n",
    "    print(\"Img index: {}\".format(index))\n",
    "    print(\"Img id: {}\".format(img_id))\n",
    "    print(\"y_pred: {}\".format(Counter(y_pred)))\n",
    "    G, df, individual_data, img = make_info(img_id)\n",
    "\n",
    "    print(df.shape[0])\n",
    "    print(len(y_pred))\n",
    "\n",
    "    # assert len(y_pred) == df.shape[0]\n",
    "    if len(y_pred) != df.shape[0]:\n",
    "        print(\"Error\")\n",
    "        continue\n",
    "\n",
    "    img2 = np.copy(img)\n",
    "    for row_index, row in df.iterrows():\n",
    "        x1, y1, x2, y2 = row[['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "        true_label = row[\"labels\"]\n",
    "\n",
    "        if isinstance(true_label, str) and true_label != \"invoice\":\n",
    "            cv2.rectangle(img2, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        _y_pred = y_pred[row_index]\n",
    "        if _y_pred != 4:\n",
    "            cv2.rectangle(img2, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "            _label = LABELS[_y_pred]\n",
    "            cv2.putText(\n",
    "                img2, \"{}\".format(_label), (x1, y1),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2\n",
    "            )\n",
    "\n",
    "    check_exist_folder(test_output_fd)\n",
    "    end = time.time()\n",
    "    print(\"\\tImage {}: {}\".format(img_id, end - start))\n",
    "    plt.imshow(img2)\n",
    "    cv2.imwrite(os.path.join(test_output_fd, '{}.jpg'.format(img_id)), img2)\n",
    "\n",
    "\n",
    "    # plt.savefig(os.path.join(test_output_fd, '{}_result.png'.format(img_id)), bbox_inches='tight')\n",
    "    # plt.savefig('{}_result.png'.format(img_id), bbox_inches='tight')\n",
    "\n",
    "# /home/long/Desktop/TestInProject/data_vaipe/public_test/prescription/image/VAIPE_P_TEST_10.png\n",
    "# /home/long/Desktop/TestInProject/data_vaipe/public_test/prescription/image/VAIPE_P_TRAIN_10.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
